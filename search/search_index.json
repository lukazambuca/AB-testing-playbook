{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"home/","title":"About us","text":"<p>This page is the home page</p>"},{"location":"chapter_0/casual_inference/","title":"Intro to A/B testing","text":"<p>In this chapter we give a brief overview of A/B testing</p>"},{"location":"chapter_0/casual_inference/#what-is-ab-testing","title":"What is A/B testing?","text":"<p>A/B testing is a method used online to compare a new product or feature with the existing version. Two user groups are involved: one sees the current version (control set) and the other sees the new version (variant or experiment). The goal is to observe any differences in their responses to determine which version is \"superior\".</p> <p>As an example, imagine we have a landing page and we want to optimise the probability that the customr will go to the next page. The current landing page has the call to action button \"click here\". However, our team might hypothesis that customers would be more likely to click on a button that writes \"More info\". To test this hypothesis, we would direct half of our traffic to the intial landing page (control), and the other half to the new version (variant)</p> <p></p>"},{"location":"chapter_0/casual_inference/#brief-history-of-ab-tests","title":"Brief History of A/B tests","text":"<p>A/B testing, often termed split testing, has its origins in the experimental designs and statistical hypothesis testing of the early 20th century, pioneered by British statistician and biologist Ronald A. Fisher. His foundational work in agriculture involved controlled experiments to assess the impacts of various treatments on crop yields. As the century progressed, these principles expanded into fields like psychology, medicine, and notably, marketing. By the dawn of the internet era in the late 20th century, the digital landscape became fertile ground for A/B testing in its contemporary form. Tech giants of the early 2000s, such as Google, Amazon, and Microsoft, harnessed A/B testing to refine their websites, products, and campaigns\u2014Google's testing of 41 shades of blue for a button being a famous example. </p> <p>The 2010s saw the democratization of A/B testing, with tools emerging that catered to smaller businesses and individual developers. However, its widespread adoption also brought ethical dilemmas to the forefront, exemplified by controversies like Facebook's 2014 emotion manipulation study. Today, A/B testing stands as a cornerstone in digital marketing, UX design, and product development, continually adapting with technological and analytical advancements.</p>"},{"location":"chapter_1/chapter_1_overview/","title":"Overview","text":"<p>Certainly! Hypothesis testing and p-values are foundational concepts in statistics, and they play a crucial role in A/B testing. Here's a detailed overview:</p>"},{"location":"chapter_1/chapter_1_overview/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Hypothesis testing is a statistical method used to make decisions or inferences about population parameters based on a sample. It involves making an initial assumption, collecting data, and then determining how likely the observed data is, given the initial assumption.</p>"},{"location":"chapter_1/chapter_1_overview/#steps-in-hypothesis-testing","title":"Steps in Hypothesis Testing:","text":"<ol> <li>State the Hypotheses: </li> <li>Null Hypothesis (H0): Represents a statement of no effect or no difference. It's a default assumption that there's no change or no association.</li> <li> <p>Alternative Hypothesis (H1 or Ha): Represents what a researcher wants to prove. It's a statement indicating the presence of an effect or difference.</p> </li> <li> <p>Choose a Significance Level (\u03b1): </p> </li> <li> <p>Commonly set at 0.05, it's the probability of rejecting the null hypothesis when it's actually true.</p> </li> <li> <p>Select a Statistical Test: </p> </li> <li> <p>The choice depends on the nature of the data and the sample size.</p> </li> <li> <p>Compute the Test Statistic: </p> </li> <li> <p>Based on the sample data.</p> </li> <li> <p>Make a Decision: </p> </li> <li>If the test statistic falls in the critical region (determined by \u03b1), reject the null hypothesis in favor of the alternative hypothesis.</li> <li>If not, fail to reject the null hypothesis.</li> </ol>"},{"location":"chapter_1/chapter_1_overview/#p-values","title":"P-values","text":"<p>The p-value is a fundamental concept in hypothesis testing. It represents the probability of observing a test statistic as extreme as, or more extreme than, the statistic computed from the sample, given that the null hypothesis is true.</p> <ul> <li> <p>Low p-value: Suggests that the observed data is inconsistent with the null hypothesis. If the p-value is less than \u03b1 (e.g., 0.05), we reject the null hypothesis.</p> </li> <li> <p>High p-value: Indicates that the observed data is consistent with the null hypothesis. If the p-value is greater than \u03b1, we fail to reject the null hypothesis.</p> </li> </ul>"},{"location":"chapter_1/chapter_1_overview/#role-in-ab-testing","title":"Role in A/B Testing","text":"<p>A/B testing, also known as split testing, is an experimental approach to compare two versions (A and B) of a webpage, app, or other product to determine which one performs better in terms of a specific metric (e.g., click-through rate, conversion rate).</p>"},{"location":"chapter_1/chapter_1_overview/#how-hypothesis-testing-and-p-values-are-used-in-ab-testing","title":"How Hypothesis Testing and P-values are used in A/B Testing:","text":"<ol> <li>State the Hypotheses: </li> <li>H0: There's no difference in performance between version A and version B.</li> <li> <p>H1: There's a significant difference in performance between version A and version B.</p> </li> <li> <p>Run the A/B Test: </p> </li> <li> <p>Randomly assign users to either version A or version B and collect data on the metric of interest.</p> </li> <li> <p>Compute the Test Statistic: </p> </li> <li> <p>This could be a difference in proportions, means, or other relevant metrics.</p> </li> <li> <p>Calculate the p-value: </p> </li> <li> <p>Determine the probability of observing the test statistic, or something more extreme, assuming the null hypothesis is true.</p> </li> <li> <p>Make a Decision: </p> </li> <li>If the p-value is less than \u03b1 (e.g., 0.05), conclude that there's a statistically significant difference between version A and version B. Otherwise, there's no significant evidence to suggest a difference.</li> </ol>"},{"location":"chapter_1/chapter_1_overview/#importance","title":"Importance:","text":"<p>A/B testing allows businesses to make data-driven decisions. By using hypothesis testing and p-values, businesses can determine whether changes to their website or product lead to improved performance. This can lead to better user experiences, increased sales, and improved user engagement.</p>"},{"location":"chapter_1/chapter_1_overview/#conclusion","title":"Conclusion","text":"<p>Hypothesis testing and p-values provide a structured framework for making decisions based on data. In the context of A/B testing, they offer a rigorous way to determine whether observed differences between two versions are statistically significant or merely due to random chance.</p>"},{"location":"chapter_1/p_value/","title":"The p-value","text":""},{"location":"chapter_1/p_value/#introduction-to-p-values","title":"Introduction to P-values","text":"<p>In this chapter we cover the definition of the infamous P-value, explain it using a simple example, show you how to calculate the value itself, and how where it falls in the world of experiments. </p>"},{"location":"chapter_1/p_value/#too-long-dont-wanna-read","title":"Too Long; Dont Wanna Read","text":"P-value is a measure used in statistics to help you decide if your null hypothesis (the assumption that there is no effect or relationship) is likely to be true or not. It's the probability that you would observe the data you have (or more extreme data) if the null hypothesis is true.   A small P-value (typically \u2264 0.05) suggests strong evidence against the null hypothesis, so you reject the null hypothesis. A large P-value (&gt; 0.05) suggests weak evidence against the null hypothesis, so you fail to reject the null hypothesis.    Remember, P-value is not the probability that the null hypothesis is true, nor it is the probability that your alternative hypothesis is false. It's just a tool to help decide whether to reject the null hypothesis."},{"location":"chapter_1/p_value/#what-is-the-p-value","title":"What is the P-value?","text":"<p>For many years, the p-value has been widely recognized and used in statistical testing. It has played a crucial role in both scientific and business communities, in which it is primarily used to determine if a result is significant or not. Unfortunately, the P-value is probably the most misunderstood and misinterpreted statistical index. We hope to break this cycle in the following chapter.</p> <p>If you start looking around the internet for the definition of the P-value, you will find a lot mumbo jumbo that even senior analysts have a tough time unwrapping (or remembering for that matter). Don't worry, I will give you this formal defintion, so no need to scour the internet, but I will also devote a few paragraphs to explain in lay terms so that the concept sticks.  </p> Formal Definition: P-Value <p>In statistical hypothesis testing, the p-value is the probability of obtaining a result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true.</p> <p>I hope I didn't lose you with the statistical lingo - we will unpack it later, but for now we begin with a simple example. Imagine we stumble upon a silver coin in the park, and our friend asks us to perform an experiment to determine whether the coin is (1) fair, or (2) unfairly biased towards one-side. Before blindly doing this experiment, let's discuss, what are our intital thoughts and expectations? Well, surely if the coin is fair we would expect that the there is a 50% chance that the coin lands on heads or tails when flipped. On the other hand, an unfair coin, such as one that is weighed more heavily on the Heads side, would have a &gt;50% chance of landing on heads when flipped. My personal expectation in a situation like this would be that the coin is actually fair. What's your guess?</p> <p>Suppose you and I are very lazy experimenters and we just flip the coin 20 times to test if it's fair or not. After flipping it 20 times, we observe that the coin landed on heads 16 times. We are very happy with how our experiment went and confidently conclude that the coin is biased towards head.</p> Flipping the coin 20 times <p>H H H H H H H H H H T H T H H H T T T H H H        Heads = 16, Tails = 4</p> <p>But now imagine our friend says; \u201c I don\u2019t believe you, you just flipped it a certain way and the coin landed on heads 16 times by pure chance. The coin is NOT heavier on one side.\u201d Before we can go on with our day, we need to address this critique. We have to show our friend that the data (16 heads) would not be expected if it was just random or by chance. Sure, it\u2019s always\u00a0possible a fair coin lands on heads 16 times, but we can show that it would be very unlikely if indeed the coin was fair, - and that's all we can reasonably do. I'm saying it again, all you can do is show that if the coin was fair, it would be very unlikely that we saw 16 heads. Therefore, it's more likely that the coin is biased towards heads.</p> <p>Without knowing it, by doing this experiment we have also defined a null hypothesis. A null hypothesis is the starting point of all scientific discoveries because it begins and is formed with an assumption. The assumption could be that eating chocolate before a race makes you faster, cold showers increase social media screen time, or sharks are heavier than crocodiles. These might seem like absurd assumptions at first (or not .. I have never seen a post about a cold shower), but truly the only way to find the answer is to run an experiment, gather data, analyse the data, and make a conclusion - just like we did with the park coin. </p> Null hypothesis <p>The null hypothesis is a default position or the commonly accepted fact, and one that we would like to test</p> <p>In our coin example, the null hypothesis would be that coin is fair sided (50% chance of landing on Heads). The alternative here is that the coin is biased. Remember that the null hypothesis is seen as a theoretical expectation, and one that we would like to test using a data driven experiment approach. </p>"},{"location":"chapter_1/p_value/#computing-the-p-value","title":"Computing the P-value","text":"<p>Stating the null hypothesis is only half the job done. The rest relies on calculating the P-value, which is a probability. Earlier we said that the p-value is the probability of obtaining a result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. I'm sure you're yawning by now. What is this definition even saying? Let's break it down into digestible pieces</p> <p>[<code>the p-value is the probability</code>], okay so we know we're dealing with probability, which means the p-value is a number between 0 and 1. </p> <p>[<code>of obtaining a result</code>] result.. as in conclusion, or as in event. Putting the two togther, we have \"the probability of getting a result\". Well, we do this all the time! The probability of rain is 10%. This is the probability of a obtaining a result (rain), right?</p> <p>[<code>at least as extreme as the one that was actually observed</code>], \"As least as extreme\" - I would prefer to reframe this, \"As we got or more extreme\". Okay, now for the second part. The term \"Actually observed\" refers to the data collected from the experiment, like whether it rained or not, or the weight of sharks/crocodiles. Still, putting the three together doesn't quite register.</p> <p>[<code>assuming that the null hypothesis is true</code>] This is the final piece of the puzzle that shows the full picture. Sometime I wonder, why did it have to go on the end? Let's send this statement to the front. \"Assuming that the null hypothesis is true\" - this means that we should start with a mental model in which our hypothesis is true, and this is BEFORE doing the experiment or collecting data. </p> <p>Putting it all together, p-value is the probability of \u201cgetting the data that you actually got\u201d under the assumption that the null hypothesis is actually true. If this still doesn't click, let's return to our coin example. The null hypothesis is that the coin is fair (i.e., P(Heads) = 0.5). We assume the hypothesis is true before we flip the coin. After flipping the coin 20 times, the p-value is the probability of getting 16 heads (the data we actually got). In a nutshell, the P-value summarises how likely it is that we got 16 heads (or more), if the coin was actually fair.</p> <p>If the p-value from your experiment is small, it indicates that the null hypothesis is unlikely to be true. This is because P-values close to 0 indicate that the observed data is unlikely to be due to chance. At risk of sounding like a broken record, I will say it only once more. In terms of our example, the probablity of getting 16 heads or more, assuming the null is true (fair coin), is low - and therefore it is unlikely that we got 16 heads by pure chance. Something else is going on.  is the P-value acts as an index that measures the strength og evidence against the null hypothesis. In such a case, you decide to discard the null hypothesis. This outcome is referred to as a statistically significant result.</p> How small should the P-value be? <p>It is statistical convention that the null hypothesis is rejected if the p-value is smaller than or equal to the significance level, which is set at \u03b1 = 0.05</p> <p>Generally, if the P-value is lower than 0.05, we reject the null hypothesis. This outcome is referred to as a statistically significant result. It's up to the experiementer to choose the cut-off mark, but it has become rather ritualistic to use 0.05. If you have trouble remembering which decision to make with your P-value, I can share with you a slogan from my university days. It goes a little like this; <code>If the P is low, the null must go</code>. In other words, if the P-value is low, reject the null hypothesis.</p>"},{"location":"chapter_1/p_value/#example-flipping-a-coin","title":"Example: Flipping a coin","text":"<p>Let\u2019s continue with our park coin example in which we test whether coin is fair, or biased towards Heads. Written in probability notation it would be; P(H) = 0.5. We start by defining our null hypothesis!</p> Null hypothesis: Coin example <ul> <li>Null hypothesis (H0): The coin is fair, i.e. P(H) = 0.5 </li> <li>Alternative hypothesis (Ha): The coin is biased towards heads: I.e P(H) &gt; 0.5</li> </ul> <p>Now, before doing the experiment (flipping the coin) and collecting data, we need to set our level of significance. In this example, we choose our value to be 5% (\u03b1 = 0.05). This means that if the P-value is below this cut-off, we reject the null hypothesis. Next, we go ahead and run our experiment! We flip the coin 20 times in a row, and observe 16 heads.</p> <p>At this point, we want to compute the P-value, which is the probability of obtaining a result at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. Sigh.. this definition again. In OTHER words, we just want to know the probability of observing 16, 17, 18, 19 or 20 heads, assumung that the coin is far (P(H) = 0.5). The P-value is therefore the P(16 heads) + P(17 heads) + P(18 heads) + P(19 heads) + P(20 heads). To calculate the P(H) we use the binomial probability distribution with n =10 and p=0.5. </p> binomial_test_one_sided.py<pre><code>from scipy.stats import binomtest\n# Binomial test\np_value = binomtest(16, 20, 0.5, alternative='greater')\nprint(p_value)\n</code></pre> Output: BinomTestResult(k=16, n=20, alternative='greater', statistic=0.8, pvalue=0.005908966064453125)  <p>Since the p-value calculated 0.0059 is lower than our cut-off of 0.05, we reject our null hypothesis that the coin is fair. </p>"},{"location":"chapter_1/p_value/#one-sided-test","title":"One-sided test","text":"<p>In the previous example, the null hypotheses was H0: coin is fair (p=0.5) and the alternative hypothesis was Ha: coin is biased toward heads (p&gt;0.5). With this hypothesis and flipping the coin 20 times, the null hypothesis would only rejected if the number of heads in 20 coin tosses was some number greater than 15. The 'rejection region' (shown as the blue bars in the graphs below) lies in the right tail of the distribution of the number of heads in 20 tosses of a coin. This type of experiment is called a one-sided test becasuse the alternative has the 'greater than' symbol'.</p> <p></p>"},{"location":"chapter_1/p_value/#two-sided-test","title":"Two sided test","text":"<p>On the other hand if you were testing H0: coin is fair (p=0.5) against the alternative hypothesis Ha: coin is not fair (p not equal to 0.5), you would reject the null hypothesis in favor of the alternative hypothesis if the number of heads was some number more than 15 or some number less than 5.  For example, we migh reject H0 in favour of the alternative if the number of heads was 3 or 18. If the alternative hypothesis has the not equals sign, then the rejection region would lie in both tails of the probability distribution of the number of heads.  This is shown by the shaded portion of the graph.  This is a two-tail test with rejection regions in both tails.</p> <p></p> binomial_test_two_sided.py<pre><code># Two-sided Binomial test\np_value = binomtest(16, 20, 0.5, alternative='two-sided')\nprint(p_value)\n</code></pre> Output: BinomTestResult(k=16, n=20, alternative='two-sided', statistic=0.8, pvalue=0.01181793212890625)  <p>However, note that when you use a significance level of 0.05 in a two-tailed test, you're splitting the 0.05 into two parts: 0.025 for each direction (or tail) of the test. This means you're checking for extreme values in both directions, with each direction having a 2.5% chance of occurring by random chance.</p>"},{"location":"chapter_1/p_value/#qa","title":"Q&amp;A","text":""},{"location":"chapter_1/p_value/#common-misconceptions-fighting-dolos","title":"Common misconceptions (Fighting Dolos)","text":""},{"location":"chapter_1/probability/","title":"Getting started","text":"<p>dlhslfhlja</p>"},{"location":"chapter_1/probability/#hello","title":"hello","text":"<p>sqfhlqhf</p>"}]}